"""
Wikiæ‰¹é‡çˆ¬å–æ¨¡å—
æ”¯æŒè‡ªåŠ¨é€’å½’è·å–Wikiçš„æ‰€æœ‰å­é¡µé¢
"""
import os
import re
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from feishu_api import FeishuAPI


class WikiCrawler:
    """Wikiæ‰¹é‡çˆ¬å–å™¨"""
    
    def __init__(self, api: FeishuAPI, export_formats: List[str] = None):
        """
        åˆå§‹åŒ–Wikiçˆ¬å–å™¨
        
        Args:
            api: FeishuAPIå®ä¾‹
            export_formats: å¯¼å‡ºæ ¼å¼åˆ—è¡¨ï¼Œå¦‚ ['md', 'docx', 'pdf']
        """
        self.api = api
        self.logger = logging.getLogger(__name__)
        self.crawled_nodes = set()  # è®°å½•å·²çˆ¬å–çš„èŠ‚ç‚¹ï¼Œé¿å…é‡å¤
        self.export_formats = export_formats or ['md']
    
    def extract_space_id_from_link(self, wiki_link: str) -> Optional[str]:
        """
        ä»Wikié“¾æ¥ä¸­æå–space_id
        
        æ”¯æŒçš„é“¾æ¥æ ¼å¼ï¼š
        1. https://xxx.feishu.cn/wiki/space/7349729703127482369?xxx (åˆ†äº«é“¾æ¥ï¼Œæ¨è)
        2. https://xxx.feishu.cn/wiki/space/7349729703127482369/wiki/xxx
        3. https://xxx.feishu.cn/wiki/ZFKlW6SLei2vLDkZXu3cS0BSn9c (wiki token)
        
        Args:
            wiki_link: Wikié“¾æ¥
            
        Returns:
            space_id æˆ– wiki_token
        """
        # æ¨¡å¼1: ç›´æ¥åŒ…å«æ•°å­—space_idï¼ˆåˆ†äº«çŸ¥è¯†åº“é“¾æ¥ï¼‰
        # /wiki/space/7349729703127482369
        pattern1 = r'feishu\.cn/wiki/space/(\d+)'
        match1 = re.search(pattern1, wiki_link)
        
        if match1:
            space_id = match1.group(1)
            self.logger.info(f"âœ… ç›´æ¥æå–åˆ°space_id: {space_id}")
            return space_id
        
        # æ¨¡å¼2: åŒ…å«spaceè·¯å¾„å’Œwiki token
        # /wiki/space/xxx/wiki/token
        pattern2 = r'feishu\.cn/wiki/space/[^/]+/wiki/([a-zA-Z0-9_-]+)'
        match2 = re.search(pattern2, wiki_link)
        
        if match2:
            wiki_token = match2.group(1)
            self.logger.info(f"æå–åˆ°Wiki token: {wiki_token}")
            return wiki_token
        
        # æ¨¡å¼3: ç›´æ¥wiki token
        pattern3 = r'feishu\.cn/wiki/([a-zA-Z0-9_-]+)'
        match3 = re.search(pattern3, wiki_link)
        
        if match3:
            wiki_token = match3.group(1)
            self.logger.info(f"æå–åˆ°Wiki token: {wiki_token}")
            return wiki_token
        
        self.logger.error("æ— æ³•ä»é“¾æ¥ä¸­æå–space_idæˆ–wiki_token")
        return None
    
    def get_wiki_space_info(self, wiki_token: str) -> Optional[str]:
        """
        é€šè¿‡wiki_tokenè·å–space_id
        
        Args:
            wiki_token: Wiki token
            
        Returns:
            space_id æˆ– None
        """
        if not self.api.access_token:
            self.logger.error("è¯·å…ˆè·å–access_token")
            return None
        
        # ä½¿ç”¨wiki/v2/spacesæ¥å£è·å–spaceä¿¡æ¯
        url = f"{self.api.base_url}/wiki/v2/spaces/{wiki_token}"
        
        headers = {
            "Authorization": f"Bearer {self.api.access_token}",
            "Content-Type": "application/json; charset=utf-8"
        }
        
        try:
            self.logger.info(f"æ­£åœ¨è·å–Wiki spaceä¿¡æ¯: {wiki_token}")
            response = self.api._make_request('GET', url, headers=headers)
            
            if response and response.get("code") == 0:
                space = response.get("data", {}).get("space", {})
                space_id = space.get("space_id")
                if space_id:
                    self.logger.info(f"æˆåŠŸè·å–space_id: {space_id}")
                    return space_id
                else:
                    self.logger.error("å“åº”ä¸­æ²¡æœ‰space_id")
                    return None
            else:
                error_msg = response.get('msg', 'Unknown error') if response else 'No response'
                self.logger.error(f"è·å–spaceä¿¡æ¯å¤±è´¥: {error_msg}")
                return None
                
        except Exception as e:
            self.logger.error(f"è·å–spaceä¿¡æ¯å¼‚å¸¸: {str(e)}")
            return None
    
    def get_child_nodes(self, space_id: str, parent_node_token: str = None) -> List[Dict[str, Any]]:
        """
        è·å–å­èŠ‚ç‚¹åˆ—è¡¨
        
        Args:
            space_id: çŸ¥è¯†ç©ºé—´ID
            parent_node_token: çˆ¶èŠ‚ç‚¹tokenï¼Œä¸ºNoneæ—¶è·å–æ ¹èŠ‚ç‚¹
            
        Returns:
            å­èŠ‚ç‚¹åˆ—è¡¨
        """
        if not self.api.access_token:
            self.logger.error("è¯·å…ˆè·å–access_token")
            return []
        
        url = f"{self.api.base_url}/wiki/v2/spaces/{space_id}/nodes"
        
        headers = {
            "Authorization": f"Bearer {self.api.access_token}",
            "Content-Type": "application/json; charset=utf-8"
        }
        
        params = {
            "page_size": 50
        }
        
        if parent_node_token:
            params["parent_node_token"] = parent_node_token
        
        all_nodes = []
        page_token = None
        
        try:
            while True:
                if page_token:
                    params["page_token"] = page_token
                
                self.logger.info(f"æ­£åœ¨è·å–å­èŠ‚ç‚¹åˆ—è¡¨: parent={parent_node_token or 'root'}")
                response = self.api._make_request('GET', url, headers=headers, params=params)
                
                if not response or response.get("code") != 0:
                    self.logger.error(f"è·å–å­èŠ‚ç‚¹å¤±è´¥: {response.get('msg') if response else 'No response'}")
                    break
                
                data = response.get("data", {})
                items = data.get("items", [])
                all_nodes.extend(items)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
                page_token = data.get("page_token")
                if not data.get("has_more", False):
                    break
                
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            self.logger.info(f"è·å–åˆ° {len(all_nodes)} ä¸ªå­èŠ‚ç‚¹")
            return all_nodes
            
        except Exception as e:
            self.logger.error(f"è·å–å­èŠ‚ç‚¹å¼‚å¸¸: {str(e)}")
            return []
    
    def crawl_node(self, node: Dict[str, Any], base_path: str, space_id: str, level: int = 0) -> int:
        """
        é€’å½’çˆ¬å–èŠ‚ç‚¹åŠå…¶å­èŠ‚ç‚¹
        
        Args:
            node: èŠ‚ç‚¹ä¿¡æ¯
            base_path: ä¿å­˜åŸºç¡€è·¯å¾„
            space_id: Wikiç©ºé—´ID
            level: å½“å‰å±‚çº§
            
        Returns:
            çˆ¬å–çš„æ–‡æ¡£æ•°é‡
        """
        node_token = node.get("node_token")
        node_type = node.get("node_type")  # doc, docx, sheet, etc.
        obj_type = node.get("obj_type", "")  # å¯¹è±¡ç±»å‹
        obj_token = node.get("obj_token", "")  # å¯¹è±¡token (ç”¨äºå¯¼å‡ºAPI)
        title = node.get("title", "æœªå‘½å")
        has_child = node.get("has_child", False)
        
        # é¿å…é‡å¤çˆ¬å–
        if node_token in self.crawled_nodes:
            self.logger.info(f"èŠ‚ç‚¹å·²çˆ¬å–ï¼Œè·³è¿‡: {title}")
            return 0
        
        self.crawled_nodes.add(node_token)
        count = 0
        
        # æ¸…ç†æ–‡ä»¶å
        safe_title = self._sanitize_filename(title)
        
        # å¦‚æœæ˜¯æ–‡æ¡£ç±»å‹ï¼Œä¸‹è½½å†…å®¹
        # node_typeå¯èƒ½æ˜¯ç©ºçš„ï¼Œä¹Ÿå¯ä»¥æ£€æŸ¥obj_type
        if node_type in ["doc", "docx"] or obj_type in ["doc", "docx"]:
            self.logger.info(f"{'  ' * level}ğŸ“„ çˆ¬å–æ–‡æ¡£: {title}")
            
            # æ ‡è®°æ˜¯å¦æˆåŠŸå¯¼å‡ºäº†è‡³å°‘ä¸€ç§æ ¼å¼
            exported_any = False
            
            # è·å–æ–‡æ¡£å†…å®¹ï¼ˆä»…ç”¨äºMarkdownå¯¼å‡ºï¼‰
            # æ³¨æ„ï¼šæ—§ç‰ˆæ–‡æ¡£ï¼ˆdocï¼‰å¯èƒ½æ— æ³•è·å–å†…å®¹ï¼Œä½†ä»å¯ä»¥å¯¼å‡ºPDF/Word
            content = None
            if 'md' in self.export_formats:
                content = self.api.get_document_content(node_token)
            
            # Markdownéœ€è¦æ–‡æ¡£å†…å®¹
            if 'md' in self.export_formats:
                if content:
                    from document_converter import DocumentConverter
                    converter = DocumentConverter()
                    metadata = {"title": title}
                    markdown_text = converter.to_markdown(content, metadata)
                    file_path = os.path.join(base_path, f"{safe_title}.md")
                    self._save_markdown(file_path, markdown_text)
                    self.logger.info(f"{'  ' * level}âœ… å·²ä¿å­˜MD: {safe_title}.md")
                    exported_any = True
                else:
                    self.logger.warning(f"{'  ' * level}âš ï¸ æ— æ³•å¯¼å‡ºMarkdownï¼ˆè·å–å†…å®¹å¤±è´¥ï¼‰")
            
            # Wordå’ŒPDFä½¿ç”¨é£ä¹¦åŸç”ŸAPIå¯¼å‡ºï¼ˆä¸éœ€è¦é¢„å…ˆè·å–å†…å®¹ï¼‰
            native_formats = [fmt for fmt in self.export_formats if fmt in ['docx', 'pdf']]
            
            if native_formats:
                from feishu_native_exporter import FeishuNativeExporter
                exporter = FeishuNativeExporter(self.api)
                
                # ä½¿ç”¨obj_tokenè¿›è¡Œå¯¼å‡ºï¼ˆè¿™æ˜¯WikièŠ‚ç‚¹å¯¹åº”çš„æ–‡æ¡£tokenï¼‰
                export_token = obj_token if obj_token else node_token
                export_type = obj_type if obj_type else (node_type or "docx")
                
                # ğŸš€ æ‰¹é‡å¯¼å‡ºï¼ˆå¹¶è¡Œå¤„ç†ï¼‰- åŒæ—¶åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
                os.makedirs(base_path, exist_ok=True)
                results = exporter.export_document_batch(
                    export_token, 
                    export_type, 
                    native_formats, 
                    base_path, 
                    safe_title
                )
                
                # å¤„ç†ç»“æœ
                for fmt, (success, error) in results.items():
                    if success:
                        self.logger.info(f"{'  ' * level}âœ… å·²ä¿å­˜{fmt.upper()} (åŸç”Ÿ): {safe_title}.{fmt}")
                        exported_any = True
                    else:
                        self.logger.warning(f"{'  ' * level}âš ï¸ å¯¼å‡º{fmt.upper()}å¤±è´¥: {error}")
            
            # å¦‚æœæˆåŠŸå¯¼å‡ºäº†ä»»ä½•æ ¼å¼ï¼Œè®¡æ•°+1
            if exported_any:
                count += 1
            else:
                self.logger.warning(f"{'  ' * level}âš ï¸ æ‰€æœ‰æ ¼å¼å¯¼å‡ºå¤±è´¥: {title}")
        
        # å¦‚æœæœ‰å­èŠ‚ç‚¹ï¼Œé€’å½’çˆ¬å–
        if has_child:
            self.logger.info(f"{'  ' * level}ğŸ“ è¿›å…¥ç›®å½•: {title}")
            
            # åˆ›å»ºå­ç›®å½•
            sub_dir = os.path.join(base_path, safe_title)
            os.makedirs(sub_dir, exist_ok=True)
            
            # è·å–å­èŠ‚ç‚¹
            child_nodes = self.get_child_nodes(space_id, node_token)
            
            # é€’å½’çˆ¬å–æ¯ä¸ªå­èŠ‚ç‚¹
            for child in child_nodes:
                count += self.crawl_node(child, sub_dir, space_id, level + 1)
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return count
    
    def crawl_wiki(self, wiki_link: str, save_path: str, progress_callback=None) -> Tuple[int, str]:
        """
        çˆ¬å–æ•´ä¸ªWiki
        
        Args:
            wiki_link: Wikié“¾æ¥
            save_path: ä¿å­˜è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(message)
            
        Returns:
            (æˆåŠŸæ•°é‡, é”™è¯¯ä¿¡æ¯)
        """
        def log_progress(msg):
            self.logger.info(msg)
            if progress_callback:
                progress_callback(msg)
        
        try:
            # æå–space_idæˆ–wiki_token
            log_progress("ğŸ“‹ æ­£åœ¨è§£æWikié“¾æ¥...")
            space_id = self.extract_space_id_from_link(wiki_link)
            
            if not space_id:
                return (0, "æ— æ³•è§£æWikié“¾æ¥ï¼Œè¯·ç¡®è®¤é“¾æ¥æ ¼å¼æ­£ç¡®")
            
            # åˆ¤æ–­æ˜¯space_idè¿˜æ˜¯wiki_token
            if space_id.isdigit():
                # å·²ç»æ˜¯space_idï¼ˆçº¯æ•°å­—ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                log_progress(f"âœ… Space ID: {space_id} (ä»é“¾æ¥ç›´æ¥è·å–)")
            else:
                # æ˜¯wiki_tokenï¼Œéœ€è¦é€šè¿‡APIè·å–space_id
                log_progress(f"ğŸ“ Wiki Token: {space_id}")
                log_progress("ğŸ” æ­£åœ¨é€šè¿‡Tokenè·å–Space ID...")
                wiki_token = space_id
                space_id = self.get_wiki_space_info(wiki_token)
                
                if not space_id:
                    return (0, "æ— æ³•è·å–Wikiç©ºé—´IDï¼Œå¯èƒ½æ˜¯æƒé™ä¸è¶³æˆ–Wikiä¸å­˜åœ¨")
                
                log_progress(f"âœ… Space ID: {space_id}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = os.path.join(save_path, f"Wikiå¯¼å‡º_{int(time.time())}")
            os.makedirs(output_dir, exist_ok=True)
            log_progress(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            
            # è·å–æ ¹èŠ‚ç‚¹åˆ—è¡¨ï¼ˆä¸æŒ‡å®šparent_node_tokenè·å–æ‰€æœ‰æ ¹èŠ‚ç‚¹ï¼‰
            log_progress("ğŸ“¥ æ­£åœ¨è·å–æ–‡æ¡£åˆ—è¡¨...")
            root_nodes = self.get_child_nodes(space_id, None)
            
            if not root_nodes:
                return (0, "æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ã€‚å¯èƒ½åŸå› ï¼š\n1. è¯¥Wikiä¸ºç©º\n2. æƒé™ä¸è¶³\n3. Space IDä¸æ­£ç¡®")
            
            log_progress(f"ğŸ“Š æ‰¾åˆ° {len(root_nodes)} ä¸ªæ ¹èŠ‚ç‚¹")
            
            # é€’å½’çˆ¬å–æ‰€æœ‰èŠ‚ç‚¹
            log_progress("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–...")
            self.crawled_nodes.clear()  # æ¸…ç©ºå·²çˆ¬å–è®°å½•
            
            total_count = 0
            for i, node in enumerate(root_nodes, 1):
                title = node.get('title', 'Unknown')
                log_progress(f"[{i}/{len(root_nodes)}] å¤„ç†: {title}")
                count = self.crawl_node(node, output_dir, space_id, 0)
                total_count += count
                time.sleep(0.5)
            
            if total_count > 0:
                log_progress(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±å¯¼å‡º {total_count} ç¯‡æ–‡æ¡£")
                log_progress(f"ğŸ“‚ ä¿å­˜ä½ç½®: {output_dir}")
                return (total_count, "")
            else:
                return (0, "æ²¡æœ‰æˆåŠŸå¯¼å‡ºä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æƒé™å’Œæ–‡æ¡£ç±»å‹")
            
        except Exception as e:
            import traceback
            error_msg = f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}"
            self.logger.error(traceback.format_exc())
            log_progress(f"âŒ {error_msg}")
            return (0, error_msg)
    
    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        
        # ç§»é™¤å‰åç©ºæ ¼
        filename = filename.strip()
        
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        
        return filename or "æœªå‘½å"
    
    def _save_markdown(self, file_path: str, content: str):
        """ä¿å­˜Markdownæ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")

