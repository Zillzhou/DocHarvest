"""
å¹¶è¡ŒWikiçˆ¬å–å™¨ - æ˜¾è‘—æå‡æ‰¹é‡å¯¼å‡ºé€Ÿåº¦
ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£
"""
import os
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
from wiki_crawler import WikiCrawler


class ParallelWikiCrawler(WikiCrawler):
    """å¹¶è¡ŒWikiçˆ¬å–å™¨ - å¤šæ–‡æ¡£åŒæ—¶å¤„ç†"""
    
    def __init__(self, api, export_formats: List[str] = None, max_workers: int = 3):
        """
        åˆå§‹åŒ–å¹¶è¡Œçˆ¬å–å™¨
        
        Args:
            api: FeishuAPIå®ä¾‹
            export_formats: å¯¼å‡ºæ ¼å¼åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶è¡Œæ•°ï¼ˆå»ºè®®2-5ï¼Œå¤ªå¤šå¯èƒ½è¢«é™æµï¼‰
        """
        super().__init__(api, export_formats)
        self.max_workers = max_workers
        self.logger = logging.getLogger(__name__)
    
    def _process_single_node(self, node: Dict[str, Any], base_path: str, level: int = 0) -> int:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£èŠ‚ç‚¹ï¼ˆä¸å¤„ç†å­èŠ‚ç‚¹ï¼‰
        
        Args:
            node: èŠ‚ç‚¹ä¿¡æ¯
            base_path: ä¿å­˜è·¯å¾„
            level: å±‚çº§
            
        Returns:
            æˆåŠŸæ•°é‡ï¼ˆ0æˆ–1ï¼‰
        """
        title = node.get("title", "æœªå‘½å")
        node_token = node.get("node_token")
        obj_token = node.get("obj_token", "")
        obj_type = node.get("obj_type", "")
        node_type = node.get("node_type")
        
        safe_title = self._sanitize_filename(title)
        exported_any = False
        
        # è·å–æ–‡æ¡£å†…å®¹ï¼ˆä»…ç”¨äºMarkdownå¯¼å‡ºï¼‰
        content = None
        if 'md' in self.export_formats:
            content = self.api.get_document_content(node_token)
        
        # Markdownå¯¼å‡º
        if 'md' in self.export_formats:
            if content:
                from document_converter import DocumentConverter
                converter = DocumentConverter()
                metadata = {"title": title}
                markdown_text = converter.to_markdown(content, metadata)
                file_path = os.path.join(base_path, f"{safe_title}.md")
                self._save_markdown(file_path, markdown_text)
                self.logger.info(f"{'  ' * level}âœ… å·²ä¿å­˜MD: {safe_title}.md")
                exported_any = True
            else:
                self.logger.warning(f"{'  ' * level}âš ï¸ æ— æ³•å¯¼å‡ºMarkdownï¼ˆè·å–å†…å®¹å¤±è´¥ï¼‰")
        
        # Wordå’ŒPDFä½¿ç”¨é£ä¹¦åŸç”ŸAPIå¯¼å‡º
        native_formats = [fmt for fmt in self.export_formats if fmt in ['docx', 'pdf']]
        
        if native_formats:
            from feishu_native_exporter import FeishuNativeExporter
            exporter = FeishuNativeExporter(self.api)
            
            export_token = obj_token if obj_token else node_token
            export_type = obj_type if obj_type else (node_type or "docx")
            
            # æ‰¹é‡å¯¼å‡º
            os.makedirs(base_path, exist_ok=True)
            results = exporter.export_document_batch(
                export_token, 
                export_type, 
                native_formats, 
                base_path, 
                safe_title
            )
            
            for fmt, (success, error) in results.items():
                if success:
                    self.logger.info(f"{'  ' * level}âœ… å·²ä¿å­˜{fmt.upper()} (åŸç”Ÿ): {safe_title}.{fmt}")
                    exported_any = True
                else:
                    self.logger.warning(f"{'  ' * level}âš ï¸ å¯¼å‡º{fmt.upper()}å¤±è´¥: {error}")
        
        return 1 if exported_any else 0
    
    def _process_node_parallel(self, node: Dict[str, Any], base_path: str, space_id: str, level: int = 0) -> int:
        """
        å¹¶è¡Œå¤„ç†èŠ‚ç‚¹ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
        
        Args:
            node: èŠ‚ç‚¹ä¿¡æ¯
            base_path: ä¿å­˜è·¯å¾„
            space_id: ç©ºé—´ID
            level: å±‚çº§
            
        Returns:
            æˆåŠŸå¯¼å‡ºçš„æ–‡æ¡£æ•°é‡
        """
        node_token = node.get("node_token")
        title = node.get("title", "æœªå‘½å")
        has_child = node.get("has_child", False)
        
        # é¿å…é‡å¤
        if node_token in self.crawled_nodes:
            return 0
        self.crawled_nodes.add(node_token)
        
        count = 0
        
        # å¤„ç†å½“å‰æ–‡æ¡£ï¼ˆå¦‚æœæ˜¯æ–‡æ¡£ç±»å‹ï¼‰
        node_type = node.get("node_type")
        obj_type = node.get("obj_type", "")
        
        if node_type in ["doc", "docx"] or obj_type in ["doc", "docx"]:
            # ä½¿ç”¨çˆ¶ç±»çš„å•æ–‡æ¡£å¤„ç†æ–¹æ³•
            count += self._process_single_node(node, base_path, level)
        
        # ğŸš€ å¹¶è¡Œå¤„ç†å­èŠ‚ç‚¹ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
        if has_child:
            self.logger.info(f"{'  ' * level}ğŸ“ è¿›å…¥ç›®å½•: {title}")
            
            # åˆ›å»ºå­ç›®å½•
            safe_title = self._sanitize_filename(title)
            sub_dir = os.path.join(base_path, safe_title)
            os.makedirs(sub_dir, exist_ok=True)
            
            # è·å–å­èŠ‚ç‚¹
            child_nodes = self.get_child_nodes(space_id, node_token)
            
            if child_nodes:
                # âš¡ ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å­èŠ‚ç‚¹
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # æäº¤æ‰€æœ‰å­èŠ‚ç‚¹ä»»åŠ¡
                    future_to_node = {
                        executor.submit(
                            self._process_node_parallel, 
                            child, 
                            sub_dir, 
                            space_id, 
                            level + 1
                        ): child 
                        for child in child_nodes
                    }
                    
                    # æ”¶é›†ç»“æœ
                    for future in as_completed(future_to_node):
                        try:
                            child_count = future.result()
                            count += child_count
                        except Exception as e:
                            child = future_to_node[future]
                            self.logger.error(f"å¤„ç†èŠ‚ç‚¹å¤±è´¥ {child.get('title')}: {str(e)}")
        
        return count
    
    def crawl_wiki(self, wiki_link: str, save_path: str) -> tuple:
        """
        å¹¶è¡Œçˆ¬å–Wikiï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼‰
        
        Args:
            wiki_link: Wikié“¾æ¥
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            (æˆåŠŸæ•°é‡, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æå–space_id
            space_id = self.extract_space_id_from_link(wiki_link)
            if not space_id:
                return (0, "æ— æ³•ä»é“¾æ¥ä¸­æå–space_id")
            
            self.logger.info(f"å¼€å§‹å¹¶è¡Œçˆ¬å–Wiki: {space_id}")
            self.logger.info(f"å¹¶è¡Œæ•°: {self.max_workers} ä¸ªæ–‡æ¡£åŒæ—¶å¤„ç†")
            
            # è·å–æ ¹èŠ‚ç‚¹
            root_nodes = self.get_child_nodes(space_id, None)
            if not root_nodes:
                return (0, "æ— æ³•è·å–Wikiæ ¹èŠ‚ç‚¹")
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            import time
            output_dir = os.path.join(save_path, f"Wikiå¯¼å‡º_{int(time.time())}")
            os.makedirs(output_dir, exist_ok=True)
            
            total_count = 0
            
            # ğŸš€ å¹¶è¡Œå¤„ç†æ‰€æœ‰æ ¹èŠ‚ç‚¹
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_node = {
                    executor.submit(
                        self._process_node_parallel,
                        node,
                        output_dir,
                        space_id,
                        0
                    ): node
                    for node in root_nodes
                }
                
                for i, future in enumerate(as_completed(future_to_node), 1):
                    try:
                        node = future_to_node[future]
                        count = future.result()
                        total_count += count
                        self.logger.info(f"[{i}/{len(root_nodes)}] å®Œæˆ: {node.get('title')} ({count}ç¯‡)")
                    except Exception as e:
                        node = future_to_node[future]
                        self.logger.error(f"å¤„ç†æ ¹èŠ‚ç‚¹å¤±è´¥ {node.get('title')}: {str(e)}")
            
            self.logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±å¯¼å‡º {total_count} ç¯‡æ–‡æ¡£")
            self.logger.info(f"ğŸ“‚ ä¿å­˜ä½ç½®: {output_dir}")
            
            return (total_count, "")
            
        except Exception as e:
            error_msg = f"æ‰¹é‡çˆ¬å–å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            import traceback
            traceback.print_exc()
            return (0, error_msg)
