"""
å¼‚æ­¥é£ä¹¦å¯¼å‡ºå™¨ - æé€Ÿç‰ˆæœ¬
ä½¿ç”¨aiohttpå®ç°å¼‚æ­¥å¹¶å‘,å¤§å¹…æå‡é€Ÿåº¦
ç›®æ ‡: 100-200ç¯‡æ–‡æ¡£åœ¨60ç§’å†…å®Œæˆ
"""
import os
import time
import logging
import asyncio
import aiohttp
from typing import Optional, Dict, Any, Tuple, List


class AsyncFeishuExporter:
    """å¼‚æ­¥é£ä¹¦å¯¼å‡ºå™¨ - ä½¿ç”¨aiohttpå®ç°é«˜å¹¶å‘"""
    
    def __init__(self, api):
        """
        åˆå§‹åŒ–å¼‚æ­¥å¯¼å‡ºå™¨
        
        Args:
            api: FeishuAPIå®ä¾‹
        """
        self.api = api
        self.logger = logging.getLogger(__name__)
        self.base_url = "https://open.feishu.cn/open-apis"
        
        # é…ç½®è¿æ¥æ± 
        self.connector = None
        self.session = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - è¿›å…¥"""
        # åˆ›å»ºè¿æ¥æ±  - å…è®¸æ›´å¤šå¹¶å‘è¿æ¥
        self.connector = aiohttp.TCPConnector(
            limit=50,  # æœ€å¤§è¿æ¥æ•°
            limit_per_host=20,  # æ¯ä¸ªä¸»æœºæœ€å¤§è¿æ¥æ•°
            ttl_dns_cache=300,  # DNSç¼“å­˜æ—¶é—´
            force_close=False,  # ä¿æŒè¿æ¥
            enable_cleanup_closed=True
        )
        
        # åˆ›å»ºä¼šè¯ - é…ç½®è¶…æ—¶
        timeout = aiohttp.ClientTimeout(
            total=120,  # æ€»è¶…æ—¶
            connect=10,  # è¿æ¥è¶…æ—¶
            sock_read=30  # è¯»å–è¶…æ—¶
        )
        
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={
                "Authorization": f"Bearer {self.api.access_token}",
                "Content-Type": "application/json; charset=utf-8"
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - é€€å‡º"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()
    
    async def export_document_batch(
        self, 
        doc_token: str, 
        doc_type: str, 
        export_formats: List[str], 
        base_path: str, 
        filename: str
    ) -> Dict[str, Tuple[bool, str]]:
        """
        å¼‚æ­¥æ‰¹é‡å¯¼å‡ºæ–‡æ¡£ï¼ˆçœŸæ­£å¹¶å‘ï¼‰
        
        Args:
            doc_token: æ–‡æ¡£token
            doc_type: æ–‡æ¡£ç±»å‹
            export_formats: å¯¼å‡ºæ ¼å¼åˆ—è¡¨
            base_path: ä¿å­˜ç›®å½•
            filename: æ–‡ä»¶å
            
        Returns:
            {æ ¼å¼: (æˆåŠŸ, é”™è¯¯ä¿¡æ¯)}
        """
        if not self.session:
            return {fmt: (False, "ä¼šè¯æœªåˆå§‹åŒ–") for fmt in export_formats}
        
        # å¹¶å‘åˆ›å»ºæ‰€æœ‰å¯¼å‡ºä»»åŠ¡
        tasks = [
            self._export_single_format(doc_token, doc_type, fmt, base_path, filename)
            for fmt in export_formats
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»„è£…ç»“æœ
        output = {}
        for fmt, result in zip(export_formats, results):
            if isinstance(result, Exception):
                output[fmt] = (False, str(result))
            else:
                output[fmt] = result
        
        return output
    
    async def _export_single_format(
        self,
        doc_token: str,
        doc_type: str,
        export_format: str,
        base_path: str,
        filename: str
    ) -> Tuple[bool, str]:
        """
        å¼‚æ­¥å¯¼å‡ºå•ä¸ªæ ¼å¼
        
        Returns:
            (æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æ­¥éª¤1: åˆ›å»ºå¯¼å‡ºä»»åŠ¡
            ticket = await self._create_export_task(doc_token, doc_type, export_format)
            if not ticket:
                return (False, "åˆ›å»ºä»»åŠ¡å¤±è´¥")
            
            # æ­¥éª¤2: è½®è¯¢ä»»åŠ¡ç»“æœï¼ˆä¼˜åŒ–è½®è¯¢é—´éš”ï¼‰
            file_token = await self._query_export_result(ticket, doc_token)
            if not file_token:
                return (False, "æŸ¥è¯¢ä»»åŠ¡å¤±è´¥æˆ–è¶…æ—¶")
            
            # æ­¥éª¤3: ä¸‹è½½æ–‡ä»¶
            save_path = os.path.join(base_path, f"{filename}.{export_format}")
            success = await self._download_exported_file(file_token, save_path)
            
            if success:
                return (True, "")
            else:
                return (False, "ä¸‹è½½å¤±è´¥")
        
        except Exception as e:
            return (False, str(e))
    
    async def _create_export_task(
        self, 
        doc_token: str, 
        doc_type: str, 
        export_format: str
    ) -> Optional[str]:
        """
        å¼‚æ­¥åˆ›å»ºå¯¼å‡ºä»»åŠ¡
        
        Returns:
            ä»»åŠ¡ticketæˆ–None
        """
        url = f"{self.base_url}/drive/v1/export_tasks"
        
        type_mapping = {
            "doc": "doc",
            "docx": "docx",
            "sheet": "sheet",
            "bitable": "bitable"
        }
        
        payload = {
            "file_extension": export_format,
            "token": doc_token,
            "type": type_mapping.get(doc_type, "docx")
        }
        
        try:
            async with self.session.post(url, json=payload) as response:
                result = await response.json()
                
                if result.get("code") == 0:
                    ticket = result.get("data", {}).get("ticket")
                    self.logger.info(f"âœ“ åˆ›å»º{export_format.upper()}ä»»åŠ¡: {ticket}")
                    return ticket
                else:
                    error_msg = result.get('msg', 'Unknown error')
                    self.logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {error_msg}")
                    return None
        
        except Exception as e:
            self.logger.error(f"åˆ›å»ºä»»åŠ¡å¼‚å¸¸: {str(e)}")
            return None
    
    async def _query_export_result(
        self, 
        ticket: str, 
        doc_token: str, 
        max_wait: int = 60
    ) -> Optional[str]:
        """
        å¼‚æ­¥æŸ¥è¯¢å¯¼å‡ºç»“æœï¼ˆä¼˜åŒ–è½®è¯¢ç­–ç•¥ï¼‰
        
        Returns:
            file_tokenæˆ–None
        """
        url = f"{self.base_url}/drive/v1/export_tasks/{ticket}"
        params = {"token": doc_token}
        
        start_time = time.time()
        check_count = 0
        
        while time.time() - start_time < max_wait:
            try:
                async with self.session.get(url, params=params) as response:
                    result = await response.json()
                    
                    if result.get("code") == 0:
                        data = result.get("data", {})
                        result_data = data.get("result", data)
                        
                        job_status = result_data.get("job_status")
                        
                        # æˆåŠŸ
                        if job_status in [0, "success"]:
                            file_token = (
                                result_data.get("file_token") or 
                                result_data.get("token") or
                                result_data.get("ticket")
                            )
                            
                            if file_token and file_token.strip():
                                return file_token.strip()
                            
                            # ä»»åŠ¡æˆåŠŸä½†tokenä¸ºç©º,ç»§ç»­ç­‰å¾…
                            await asyncio.sleep(0.3)
                        
                        # å¤±è´¥
                        elif job_status in [3, "failed"]:
                            error_msg = data.get("job_error_msg", "Unknown error")
                            self.logger.error(f"å¯¼å‡ºå¤±è´¥: {error_msg}")
                            return None
                        
                        # è¿›è¡Œä¸­ - æ¿€è¿›è½®è¯¢ç­–ç•¥
                        else:
                            check_count += 1
                            if check_count <= 5:
                                await asyncio.sleep(0.2)  # å‰5æ¬¡å¿«é€Ÿæ£€æŸ¥
                            elif check_count <= 10:
                                await asyncio.sleep(0.5)  # 6-10æ¬¡ä¸­é€Ÿ
                            else:
                                await asyncio.sleep(1)    # ä¹‹åæ­£å¸¸é—´éš”
                    else:
                        self.logger.error(f"æŸ¥è¯¢å¤±è´¥: {result.get('msg')}")
                        return None
            
            except asyncio.TimeoutError:
                self.logger.warning("æŸ¥è¯¢è¶…æ—¶,é‡è¯•...")
                await asyncio.sleep(1)
            except Exception as e:
                self.logger.warning(f"æŸ¥è¯¢å¼‚å¸¸,é‡è¯•: {str(e)}")
                await asyncio.sleep(1)
        
        self.logger.error("å¯¼å‡ºè¶…æ—¶")
        return None
    
    async def _download_exported_file(
        self, 
        file_token: str, 
        save_path: str
    ) -> bool:
        """
        å¼‚æ­¥ä¸‹è½½æ–‡ä»¶
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        url = f"{self.base_url}/drive/v1/export_tasks/file/{file_token}/download"
        
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    self.logger.error(f"ä¸‹è½½å¤±è´¥: HTTP {response.status}")
                    return False
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                
                # å¼‚æ­¥å†™å…¥æ–‡ä»¶
                with open(save_path, 'wb') as f:
                    async for chunk in response.content.iter_chunked(8192):
                        f.write(chunk)
                
                self.logger.info(f"âœ“ å·²ä¸‹è½½: {os.path.basename(save_path)}")
                return True
        
        except Exception as e:
            self.logger.error(f"ä¸‹è½½å¼‚å¸¸: {str(e)}")
            return False


class AsyncParallelWikiCrawler:
    """
    å¼‚æ­¥å¹¶è¡ŒWikiçˆ¬å–å™¨ - æé€Ÿç‰ˆæœ¬
    ä½¿ç”¨å¼‚æ­¥I/O + é«˜å¹¶å‘å®ç°æè‡´æ€§èƒ½
    """
    
    def __init__(self, api, export_formats: List[str] = None, max_workers: int = 10):
        """
        Args:
            api: FeishuAPIå®ä¾‹
            export_formats: å¯¼å‡ºæ ¼å¼åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆå»ºè®®10-20ï¼‰
        """
        self.api = api
        self.export_formats = export_formats or ['pdf']
        self.max_workers = max_workers
        self.logger = logging.getLogger(__name__)
        self.crawled_nodes = set()
        self.semaphore = None  # å¹¶å‘æ§åˆ¶ä¿¡å·é‡
    
    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        filename = filename.strip()
        if len(filename) > 100:
            filename = filename[:100]
        return filename or "æœªå‘½å"
    
    async def _process_document_node(
        self, 
        node: Dict[str, Any], 
        base_path: str,
        exporter: AsyncFeishuExporter,
        level: int = 0
    ) -> int:
        """
        å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡æ¡£èŠ‚ç‚¹
        
        Returns:
            æˆåŠŸæ•°é‡ï¼ˆ0æˆ–1ï¼‰
        """
        title = node.get("title", "æœªå‘½å")
        node_token = node.get("node_token")
        obj_token = node.get("obj_token", "")
        obj_type = node.get("obj_type", "")
        node_type = node.get("node_type")
        
        safe_title = self._sanitize_filename(title)
        exported_any = False
        
        # å¤„ç†Markdownæ ¼å¼ï¼ˆåŒæ­¥è·å–å†…å®¹ï¼‰
        if 'md' in self.export_formats:
            try:
                content = self.api.get_document_content(node_token)
                if content:
                    from document_converter import DocumentConverter
                    converter = DocumentConverter()
                    metadata = {"title": title}
                    markdown_text = converter.to_markdown(content, metadata)
                    file_path = os.path.join(base_path, f"{safe_title}.md")
                    
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(base_path, exist_ok=True)
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(markdown_text)
                    
                    self.logger.info(f"{'  ' * level}âœ… MD: {safe_title}.md")
                    exported_any = True
                else:
                    self.logger.warning(f"{'  ' * level}âš ï¸ MDè·å–å†…å®¹å¤±è´¥: {title}")
            except Exception as e:
                self.logger.error(f"{'  ' * level}âŒ MDå¯¼å‡ºå¤±è´¥: {title} - {str(e)}")
        
        # å¤„ç†PDFå’ŒWordï¼ˆä½¿ç”¨å¼‚æ­¥å¯¼å‡ºå™¨ï¼‰
        native_formats = [fmt for fmt in self.export_formats if fmt in ['docx', 'pdf']]
        
        if native_formats:
            export_token = obj_token if obj_token else node_token
            export_type = obj_type if obj_type else (node_type or "docx")
            
            # ä½¿ç”¨å¼‚æ­¥å¯¼å‡ºå™¨
            os.makedirs(base_path, exist_ok=True)
            results = await exporter.export_document_batch(
                export_token,
                export_type,
                native_formats,
                base_path,
                safe_title
            )
            
            # æ£€æŸ¥ç»“æœ
            for fmt, (success, error) in results.items():
                if success:
                    self.logger.info(f"{'  ' * level}âœ… {fmt.upper()}: {safe_title}.{fmt}")
                    exported_any = True
                else:
                    self.logger.warning(f"{'  ' * level}âŒ {fmt.upper()}å¤±è´¥: {error}")
        
        if exported_any:
            return 1
        else:
            self.logger.warning(f"{'  ' * level}âŒ æ‰€æœ‰æ ¼å¼å¯¼å‡ºå¤±è´¥: {title}")
            return 0
    
    async def _crawl_node_async(
        self,
        node: Dict[str, Any],
        base_path: str,
        space_id: str,
        exporter: AsyncFeishuExporter,
        level: int = 0
    ) -> int:
        """
        å¼‚æ­¥é€’å½’çˆ¬å–èŠ‚ç‚¹
        
        Returns:
            æˆåŠŸæ–‡æ¡£æ•°
        """
        node_token = node.get("node_token")
        title = node.get("title", "æœªå‘½å")
        has_child = node.get("has_child", False)
        
        # é¿å…é‡å¤
        if node_token in self.crawled_nodes:
            return 0
        self.crawled_nodes.add(node_token)
        
        count = 0
        
        # å¤„ç†å½“å‰æ–‡æ¡£
        node_type = node.get("node_type")
        obj_type = node.get("obj_type", "")
        
        if node_type in ["doc", "docx"] or obj_type in ["doc", "docx"]:
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            async with self.semaphore:
                count += await self._process_document_node(node, base_path, exporter, level)
        
        # å¤„ç†å­èŠ‚ç‚¹
        if has_child:
            safe_title = self._sanitize_filename(title)
            sub_dir = os.path.join(base_path, safe_title)
            os.makedirs(sub_dir, exist_ok=True)
            
            # åŒæ­¥è·å–å­èŠ‚ç‚¹ï¼ˆè¿™éƒ¨åˆ†APIä¸æ”¯æŒå¼‚æ­¥ï¼‰
            from wiki_crawler import WikiCrawler
            temp_crawler = WikiCrawler(self.api, self.export_formats)
            child_nodes = temp_crawler.get_child_nodes(space_id, node_token)
            
            if child_nodes:
                # å¼‚æ­¥å¹¶å‘å¤„ç†æ‰€æœ‰å­èŠ‚ç‚¹
                tasks = [
                    self._crawl_node_async(child, sub_dir, space_id, exporter, level + 1)
                    for child in child_nodes
                ]
                child_counts = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in child_counts:
                    if isinstance(result, int):
                        count += result
                    else:
                        self.logger.error(f"å­èŠ‚ç‚¹å¤„ç†å¤±è´¥: {result}")
        
        return count
    
    async def crawl_wiki_async(self, wiki_link: str, save_path: str) -> Tuple[int, str]:
        """
        å¼‚æ­¥çˆ¬å–Wiki
        
        Returns:
            (æˆåŠŸæ•°é‡, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æå–space_id
            from wiki_crawler import WikiCrawler
            temp_crawler = WikiCrawler(self.api, self.export_formats)
            space_id = temp_crawler.extract_space_id_from_link(wiki_link)
            
            if not space_id:
                return (0, "æ— æ³•æå–space_id")
            
            self.logger.info(f"ğŸš€ å¼€å§‹æé€Ÿçˆ¬å–: {space_id}")
            self.logger.info(f"âš¡ å¹¶å‘æ•°: {self.max_workers}")
            self.logger.info(f"ğŸ“¤ æ ¼å¼: {', '.join(self.export_formats)}")
            
            # è·å–æ ¹èŠ‚ç‚¹
            root_nodes = temp_crawler.get_child_nodes(space_id, None)
            if not root_nodes:
                return (0, "æ— æ³•è·å–æ ¹èŠ‚ç‚¹")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = os.path.join(save_path, f"Wikiå¯¼å‡º_{int(time.time())}")
            os.makedirs(output_dir, exist_ok=True)
            
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            self.semaphore = asyncio.Semaphore(self.max_workers)
            
            # ä½¿ç”¨å¼‚æ­¥å¯¼å‡ºå™¨
            async with AsyncFeishuExporter(self.api) as exporter:
                # å¹¶å‘å¤„ç†æ‰€æœ‰æ ¹èŠ‚ç‚¹
                tasks = [
                    self._crawl_node_async(node, output_dir, space_id, exporter, 0)
                    for node in root_nodes
                ]
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                total_count = 0
                for result in results:
                    if isinstance(result, int):
                        total_count += result
                    else:
                        self.logger.error(f"æ ¹èŠ‚ç‚¹å¤„ç†å¤±è´¥: {result}")
            
            self.logger.info(f"ğŸ‰ å®Œæˆ! å…± {total_count} ç¯‡æ–‡æ¡£")
            self.logger.info(f"ğŸ“‚ ä½ç½®: {output_dir}")
            
            return (total_count, "")
        
        except Exception as e:
            error_msg = f"çˆ¬å–å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            import traceback
            traceback.print_exc()
            return (0, error_msg)
    
    def crawl_wiki(self, wiki_link: str, save_path: str) -> Tuple[int, str]:
        """
        åŒæ­¥åŒ…è£…å™¨ - è¿è¡Œå¼‚æ­¥çˆ¬å–
        
        Returns:
            (æˆåŠŸæ•°é‡, é”™è¯¯ä¿¡æ¯)
        """
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            return loop.run_until_complete(
                self.crawl_wiki_async(wiki_link, save_path)
            )
        finally:
            loop.close()
